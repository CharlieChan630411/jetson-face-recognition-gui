#!/usr/bin/env python3
"""convert2trt.py â€“ RetinaFace ONNX âœ TensorRT è½‰æ›å·¥å…·ï¼ˆæ”¯æ´ TensorRT 10+ï¼‰

â˜… 2025-07-07 æ›´æ–°ï¼š
  - âœ… æ”¯æ´ TensorRT v10 API (`build_serialized_network + deserialize_cuda_engine`)
  - âœ… ç›¸å®¹èˆŠç‰ˆ API (`build_engine`)ï¼šæœƒè‡ªå‹•åµæ¸¬
  - âœ… å¯è·³é pycudaï¼Œç´”ç”¨ TensorRT ç·¨è­¯å¼•æ“

ä½¿ç”¨ç¯„ä¾‹ï¼š
    python3 convert2trt.py \
        --onnx retinaface.onnx \
        --engine retinaface.engine \
        --input-shape 1x3x640x640 \
        --fp16
"""
from __future__ import annotations
import argparse
import os
import sys
import tensorrt as trt

# ------------------ PyCUDA (Optional) ------------------
try:
    import pycuda.autoinit  # noqa: F401 â€“ åˆå§‹åŒ– CUDA context
    import pycuda.driver as cuda  # noqa: F401
except ModuleNotFoundError:
    print("âš ï¸ åµæ¸¬åˆ°ç³»çµ±æœªå®‰è£ pycudaï¼›å°‡ä»¥ TensorRT å…§å»º Driver å»ºæ§‹å¼•æ“ï¼Œé€™åœ¨ *ç´”å»ºæ§‹* éšæ®µå®Œå…¨æ²’å•é¡Œã€‚\n"
          "   å¦‚éœ€æ—¥å¾Œåœ¨ Python å…§ç›´æ¥æ¨è«–ä¸¦ç®¡ç† GPU bufferï¼Œä»å»ºè­° `sudo pip3 install pycuda` æˆ–ä½¿ç”¨ apt å®‰è£ `python3-pycuda`ã€‚")

TRT_LOGGER = trt.Logger(trt.Logger.INFO)

# -------------------------- Helper å‡½å¼ -------------------------- #

def _parse_shape(shape_str: str) -> tuple[int, int, int, int]:
    try:
        n, c, h, w = map(int, shape_str.lower().split("x"))
        return n, c, h, w
    except Exception as e:
        raise argparse.ArgumentTypeError(
            f"Shape å¿…é ˆç‚º NxCxHxWï¼Œä¾‹å¦‚ 1x3x640x640ï¼›æ”¶åˆ°: {shape_str}") from e


def _add_builder_config(builder: trt.Builder, fp16: bool, max_ws: int) -> trt.BuilderConfig:
    config = builder.create_builder_config()
    ws_bytes = max_ws * (1 << 20)
    if hasattr(config, "set_memory_pool_limit"):
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, ws_bytes)
    else:
        config.max_workspace_size = ws_bytes
    if fp16 and builder.platform_has_fast_fp16:
        config.set_flag(trt.BuilderFlag.FP16)
    return config

# -------------------------- ä¸»è¦æµç¨‹ -------------------------- #

def build_engine(
    onnx_path: str,
    engine_path: str,
    min_shape: tuple[int, int, int, int],
    opt_shape: tuple[int, int, int, int],
    max_shape: tuple[int, int, int, int],
    fp16: bool,
    max_ws_mb: int = 2048,
) -> None:
    if not os.path.exists(onnx_path):
        sys.exit(f"âŒ æ‰¾ä¸åˆ° ONNX æª”æ¡ˆï¼š{onnx_path}")

    with trt.Builder(TRT_LOGGER) as builder, \
         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \
         trt.OnnxParser(network, TRT_LOGGER) as parser:

        print(f"ğŸ“– è®€å– ONNXï¼š{onnx_path}")
        with open(onnx_path, "rb") as f:
            if not parser.parse(f.read()):
                print("âŒ è§£æ ONNX å¤±æ•—ï¼š")
                for i in range(parser.num_errors):
                    print(parser.get_error(i))
                sys.exit(1)

        if network.num_outputs == 0:
            sys.exit("âŒ æ¨¡å‹æ²’æœ‰è¼¸å‡ºç¯€é»ï¼Œè«‹æª¢æŸ¥ ONNX åŒ¯å‡ºæµç¨‹ï¼")

        print("ğŸ”§ å»ºç«‹ Optimization Profile â€¦")
        profile = builder.create_optimization_profile()
        input_tensor = network.get_input(0)
        profile.set_shape(input_tensor.name, min_shape, opt_shape, max_shape)

        config = _add_builder_config(builder, fp16, max_ws_mb)
        config.add_optimization_profile(profile)

        print("ğŸš§ é–‹å§‹å»ºæ§‹ TensorRT å¼•æ“ â€¦")

        if hasattr(builder, "build_serialized_network"):
            # TensorRT 8.5+ ç”¨æ³•
            serialized_engine = builder.build_serialized_network(network, config)
            if serialized_engine is None:
                sys.exit("âŒ å¼•æ“åºåˆ—åŒ–å¤±æ•—ï¼")
            runtime = trt.Runtime(TRT_LOGGER)
            engine = runtime.deserialize_cuda_engine(serialized_engine)
        else:
            # èˆŠç‰ˆ API
            engine = builder.build_engine(network, config)

        if engine is None:
            sys.exit("âŒ å¼•æ“å»ºæ§‹å¤±æ•—ï¼")

        with open(engine_path, "wb") as f:
            f.write(engine.serialize())
        print(f"âœ… æˆåŠŸå„²å­˜å¼•æ“ï¼š{engine_path}")


# -------------------------- CLI -------------------------- #

def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="ONNX âœ TensorRT è½‰æ›è…³æœ¬ (RetinaFace å°ˆç”¨)")
    p.add_argument("--onnx", required=True, help="ONNX æ¨¡å‹è·¯å¾‘")
    p.add_argument("--engine", required=True, help="è¼¸å‡º TensorRT engine æª”å")
    p.add_argument("--input-shape", default="1x3x640x640", type=_parse_shape,
                   help="æœ€å°è¼¸å…¥å°ºå¯¸ (min_shape)ï¼Œé è¨­ 1x3x640x640")
    p.add_argument("--opt-shape", type=_parse_shape,
                   help="æœ€ä½³è¼¸å…¥å°ºå¯¸ (opt_shape)ï¼Œé è¨­åŒ --input-shape")
    p.add_argument("--max-shape", type=_parse_shape,
                   help="æœ€å¤§è¼¸å…¥å°ºå¯¸ (max_shape)ï¼Œé è¨­åŒ --opt-shape")
    p.add_argument("--fp16", action="store_true", help="å•Ÿç”¨ FP16")
    p.add_argument("--workspace", type=int, default=2048,
                   help="æœ€å¤§å·¥ä½œç©ºé–“ (MB)ï¼Œé è¨­ 2048MB")
    return p.parse_args()


if __name__ == "__main__":
    args = parse_args()
    opt_shape = args.opt_shape or args.input_shape
    max_shape = args.max_shape or opt_shape
    build_engine(
        onnx_path=args.onnx,
        engine_path=args.engine,
        min_shape=args.input_shape,
        opt_shape=opt_shape,
        max_shape=max_shape,
        fp16=args.fp16,
        max_ws_mb=args.workspace,
    )
